# SQL文件分割工具

这是一个用于将大型SQL文件分割成多个小文件的Python工具。该工具特别适合处理大型SQL文件，采用流式处理方式，避免内存溢出问题。

## 功能特点

- 支持将大型SQL文件分割成多个小文件
- 智能识别SQL语句，避免错误分割
- 自动跳过SQL注释
- 可自定义每个文件包含的SQL语句数量
- 支持UTF-8编码
- 自动创建输出目录
- 支持处理超大SQL文件（采用流式处理）
- 文件大小检查和警告
- 完善的错误处理机制
- 实时进度显示
- 处理速度统计
- 内存使用监控
- 预计剩余时间显示

## 安装要求

- Python 3.6 或更高版本
- 依赖包：
  - psutil (用于监控内存使用)

### 安装依赖

```bash
pip install psutil
```

## 使用方法

### 基本用法

```bash
python sql_splitter.py input.sql output_directory
```

### 自定义每个文件的SQL语句数量

```bash
python sql_splitter.py input.sql output_directory --statements 50
```

### 参数说明

- `input.sql`: 输入的SQL文件路径
- `output_directory`: 输出目录路径
- `--statements`: 每个文件包含的SQL语句数量（可选，默认为100）

## 进度显示

工具会实时显示以下信息：
- 处理进度百分比
- 已处理文件大小
- 总文件大小
- 处理速度
- 预计剩余时间
- 内存使用情况

## 输出文件命名规则

分割后的文件将按照以下格式命名：
```
原文件名_part001.sql
原文件名_part002.sql
原文件名_part003.sql
...
```

## 示例

假设有一个名为`large.sql`的文件，要将其分割成每个文件包含50条SQL语句的小文件：

```bash
python sql_splitter.py large.sql output --statements 50
```

这将在`output`目录下创建多个SQL文件，每个文件包含最多50条SQL语句。

## 大文件处理

该工具特别适合处理大型SQL文件：

- 采用流式处理方式，避免内存溢出
- 自动检测文件大小，对超大文件（>1GB）发出警告
- 支持处理任意大小的SQL文件
- 内存使用量保持稳定，不会随文件大小增长
- 实时显示处理进度和性能指标

## 注意事项

1. 确保有足够的磁盘空间存储分割后的文件
2. 输入文件必须是有效的SQL文件
3. 输出目录如果不存在会自动创建
4. 如果输出目录中已存在同名文件，将会被覆盖
5. 对于超大文件（>1GB），建议适当减小每个文件的SQL语句数量
6. 处理大文件时，请关注内存使用情况

## 错误处理

- 如果输入文件不存在，程序会报错并退出
- 如果无法创建输出目录，程序会报错并退出
- 如果输入文件不是有效的SQL文件，可能会导致分割结果不准确
- 所有错误都会提供清晰的错误信息

## 性能优化

- 使用流式处理，避免一次性加载整个文件
- 优化内存使用，定期清理不需要的数据
- 支持处理超大文件
- 提供处理进度信息
- 实时监控内存使用情况

## 贡献

欢迎提交Issue和Pull Request来改进这个工具。 